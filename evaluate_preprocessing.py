import os
import sys
import argparse
import torch
import numpy as np
import random
from pathlib import Path
from collections import defaultdict
import matplotlib.pyplot as plt # å¯é€‰ï¼šç”¨äºç”»åˆ†å¸ƒå›¾

# === è·¯å¾„æ³¨å…¥ (æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´) ===
current_file = Path(__file__).resolve()
project_root = current_file.parent
sys.path.append(str(project_root))

# å°è¯•å¼•å…¥å¿…è¦ç»„ä»¶ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•ä»…ä½¿ç”¨ Torch
try:
    from starrygl.data.batch import AtomicBatch
    from starrygl.cache.route import CommPlan
except ImportError:
    print("[Warning] StarryGL modules not found. Using raw torch.load (might fail on custom classes).")

class PreprocessingEvaluator:
    def __init__(self, data_root, dataset, num_parts):
        self.root = Path(data_root)
        self.dataset = dataset
        self.num_parts = num_parts
        self.suffix = f"{dataset}_{num_parts:03d}"
        
        # è·¯å¾„å®šä¹‰
        self.nparts_dir = self.root / "nparts" 
        self.processed_dir_base = self.root / "processed_atomic" / self.suffix
        
        # æŸ¥æ‰¾çœŸå®çš„ nparts ç›®å½• (å¤„ç†å¯èƒ½çš„åç¼€)
        candidates = list(self.nparts_dir.glob(f"{self.suffix}*"))
        if not candidates:
            raise FileNotFoundError(f"Cannot find nparts directory for {self.suffix}")
        self.meta_dir = candidates[0]
        
        print(f"âœ… Target Metadata: {self.meta_dir}")
        print(f"âœ… Target Chunks: {self.processed_dir_base}")
        print("-" * 60)

    def load_partition_book(self):
        """åŠ è½½ Partition Bookï¼Œå…¼å®¹å¤šç§å­˜å‚¨æ ¼å¼"""
        pb_path = self.meta_dir / "partition_book.pt"
        data = torch.load(pb_path, map_location='cpu')
        
        # å…¼å®¹ tuple (book, local_ids...) æˆ– list æˆ– ç›´æ¥ tensor list
        if isinstance(data, (tuple, list)):
            return data[0]
        return data

    def eval_load_balance(self):
        """
        1. è´Ÿè½½å‡è¡¡è¯„ä¼°
        - Computation Balance: Owned Nodes åˆ†å¸ƒ
        - Memory Balance: Total Stored (Owned + Halo) åˆ†å¸ƒ
        """
        print("\nğŸ“Š [Metric 1] Load Balancing Analysis")
        p_book = self.load_partition_book()
        
        owned_counts = []
        stored_counts = []
        
        for rank in range(self.num_parts):
            # 1. Owned Nodes
            n_owned = len(p_book[rank])
            owned_counts.append(n_owned)
            
            # 2. Stored Nodes (ä» distributed_context è¯»å–)
            ctx_path = self.meta_dir / f"part_{rank}" / "distributed_context.pt"
            if ctx_path.exists():
                # åªè¯» Metadataï¼Œä¸åŠ è½½å¤§ Tensor
                # è¿™ç§ trick ä¾èµ–äº pytorch ç‰ˆæœ¬ï¼Œå¦‚æœæ˜¯ zip æ ¼å¼é€šå¸¸éœ€è¦åŠ è½½
                # è¿™é‡Œæˆ‘ä»¬å®Œæ•´åŠ è½½ä½†ç«‹å³é‡Šæ”¾
                ctx = torch.load(ctx_path, map_location='cpu')
                if 'node_feat' in ctx:
                    n_stored = ctx['node_feat'].shape[0]
                else:
                    # å¦‚æœæ²¡æœ‰ featureï¼Œå‡è®¾ stored = map é•¿åº¦
                    # è¿™é‡Œå‡è®¾ context é‡Œæœ‰ local_map
                    n_stored = n_owned # Fallback
                stored_counts.append(n_stored)
            else:
                print(f"  [Warn] Context missing for rank {rank}")
                stored_counts.append(n_owned)

        owned = np.array(owned_counts)
        stored = np.array(stored_counts)
        
        # è®¡ç®—æŒ‡æ ‡
        print(f"  > Computation Load (Owned Nodes):")
        print(f"    - Mean: {owned.mean():.1f}")
        print(f"    - Std Dev: {owned.std():.1f}")
        print(f"    - CV (Coeff of Variation): {owned.std()/owned.mean():.4f} (Ideal: 0.0)")
        
        print(f"  > Memory Load (Stored Nodes):")
        print(f"    - Mean: {stored.mean():.1f}")
        print(f"    - Max/Mean Ratio: {stored.max()/stored.mean():.4f} (Ideal: 1.0)")
        
        return stored.sum() # è¿”å›æ€»ç‰©ç†å­˜å‚¨é‡ä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨

    def eval_communication(self, total_stored_nodes):
        """
        2. é€šä¿¡å¼€é”€è¯„ä¼°
        - Replication Factor (RF)
        - Halo Ratio
        """
        print("\nğŸ“¡ [Metric 2] Communication Cost")
        
        p_book = self.load_partition_book()
        # è¿‘ä¼¼å…¨å±€èŠ‚ç‚¹æ•° (å‡è®¾ partition è¦†ç›–å…¨å›¾ä¸”äº’æ–¥)
        total_unique_nodes = sum([len(b) for b in p_book])
        
        # Replication Factor = æ€»å­˜å‚¨èŠ‚ç‚¹æ•° / å®é™…å”¯ä¸€èŠ‚ç‚¹æ•°
        rf = total_stored_nodes / max(1, total_unique_nodes)
        
        # Halo Ratio = Halo / Stored
        total_halo = total_stored_nodes - total_unique_nodes
        halo_ratio = total_halo / max(1, total_stored_nodes)
        
        print(f"  - Total Unique Nodes: {total_unique_nodes}")
        print(f"  - Total Physical Nodes (Sum over ranks): {total_stored_nodes}")
        print(f"  > Replication Factor (RF): {rf:.4f}")
        print(f"    (Interpretation: Each node is stored on {rf:.2f} GPUs on average)")
        print(f"  > Avg Halo Ratio: {halo_ratio:.2%}")
        
        if rf > 2.0:
            print("    âš ï¸ [Alert] High RF detected! Check partitioning algorithm.")
        else:
            print("    âœ… [Pass] RF is within acceptable range (< 2.0).")

    def eval_temporal_integrity(self, sample_ratio=0.1):
        """
        3. æ—¶åºä¸€è‡´æ€§æ£€æŸ¥
        - æ£€æŸ¥ Batch é—´çš„æ—¶é—´å•è°ƒæ€§
        - æ£€æŸ¥ Batch å†…çš„ä¿¡æ¯æ³„éœ² (Edge TS > Batch TS)
        """
        print("\nâ³ [Metric 3] Temporal Integrity Check")
        
        # æ‰«æä»»ä¸€åˆ†åŒºçš„ chunk æ–‡ä»¶
        chunk_dir = self.processed_dir_base / "part_0"
        if not chunk_dir.exists():
            print(f"  [Error] Chunk dir not found: {chunk_dir}")
            return

        files = sorted(list(chunk_dir.glob("slot_*.pt")), key=lambda x: x.name)
        num_files = len(files)
        if num_files == 0:
            print("  [Error] No slot files found.")
            return
            
        # æŠ½æ ·
        indices = sorted(random.sample(range(num_files), max(1, int(num_files * sample_ratio))))
        print(f"  - Scanning {len(indices)}/{num_files} files for violations...")
        
        violations = 0
        leakages = 0
        prev_max_ts = -1.0
        
        for idx in indices:
            f = files[idx]
            try:
                # åŠ è½½ AtomicBatch
                raw = torch.load(f, map_location='cpu')
                # å…¼å®¹æ ¼å¼: å¯èƒ½æ˜¯ AtomicBatch å¯¹è±¡ï¼Œä¹Ÿå¯èƒ½æ˜¯ list
                if hasattr(raw, 'task_data'):
                    task = raw.task_data
                    layers = raw.layer_data
                elif isinstance(raw, list):
                    task = raw[0]
                    layers = raw[1:]
                else:
                    continue

                # 1. æ£€æŸ¥ Batch æ—¶é—´èŒƒå›´
                # å‡è®¾ task['ts'] æˆ– task['task_ts'] å­˜åœ¨
                ts_key = 'task_ts' if 'task_ts' in task else 'ts'
                if ts_key not in task:
                    # å¯èƒ½æ˜¯ task_start / task_end
                    current_min = task.get('time_start', 0)
                    current_max = task.get('time_end', 0)
                else:
                    ts_tensor = task[ts_key]
                    current_min = ts_tensor.min().item()
                    current_max = ts_tensor.max().item()
                
                if current_min < prev_max_ts:
                    violations += 1
                prev_max_ts = current_max
                
                # 2. æ£€æŸ¥å­å›¾æ³„éœ² (Leakage)
                # ä»»ä½•å­å›¾è¾¹çš„ç”Ÿæˆæ—¶é—´ä¸å¾—æ™šäº Task çš„å‘ç”Ÿæ—¶é—´
                for layer in layers:
                    if 'edge_ts' in layer:
                        e_max = layer['edge_ts'].max().item()
                        if e_max > current_max:
                            leakages += 1
                            
            except Exception as e:
                print(f"  [Error reading {f.name}]: {e}")
                continue

        if violations == 0 and leakages == 0:
            print("  âœ… [Pass] No temporal violations or leakages detected.")
        else:
            print(f"  âŒ [Fail] Found {violations} order violations and {leakages} information leakages.")

    def eval_route_validity(self):
        """
        4. è·¯ç”±è¦†ç›–ç‡æ£€æŸ¥
        - ç»Ÿè®¡é€šä¿¡é‡ï¼Œç¡®ä¿ä¸æ˜¯ 0 (æ­»è·¯ç”±)
        """
        print("\nğŸ›£ï¸ [Metric 4] Route Validity Check")
        
        chunk_dir = self.processed_dir_base / "part_0"
        files = list(chunk_dir.glob("slot_*.pt"))[:5] # åªæ£€æŸ¥å‰5ä¸ª
        
        total_send = 0
        has_route = False
        
        for f in files:
            raw = torch.load(f, map_location='cpu')
            
            # æå– comm_plans
            plans = []
            if hasattr(raw, 'comm_plans'): plans = raw.comm_plans
            elif isinstance(raw, list):
                # å°è¯•ä» list[0] (task_data) ä¸­æ‰¾
                task = raw[0]
                plans = task.get('comm_plans', task.get('comm_plan', []))
            
            if not isinstance(plans, list): plans = [plans]
            
            for p in plans:
                if p is not None:
                    has_route = True
                    # send_sizes: [world_size]
                    if hasattr(p, 'send_sizes'):
                        total_send += p.send_sizes.sum().item()
        
        print(f"  - Route Object Found: {has_route}")
        print(f"  - Sampled Communication Volume: {int(total_send)} items")
        
        if total_send == 0 and has_route:
            print("  âš ï¸ [Warning] Route exists but communication volume is ZERO. (Is this a single-partition run?)")
        elif total_send > 0:
            print("  âœ… [Pass] Valid communication traffic detected.")

    def run(self):
        print(f"ğŸš€ Starting Evaluation for {self.dataset} ({self.num_parts} Partitions)")
        
        # 1. Load Balance
        total_stored = self.eval_load_balance()
        
        # 2. Communication
        self.eval_communication(total_stored)
        
        # 3. Temporal
        self.eval_temporal_integrity()
        
        # 4. Route
        self.eval_route_validity()
        
        print("\nâœ¨ Evaluation Complete.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-root", type=str, default="/mnt/data/zlj/starrygl-data/")
    parser.add_argument("--dataset", type=str, default="WIKI")
    parser.add_argument("--parts", type=int, default=4)
    args = parser.parse_args()
    
    evaluator = PreprocessingEvaluator(args.data_root, args.dataset, args.parts)
    evaluator.run()